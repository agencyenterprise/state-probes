# PyTorch 2.x Migration Summary

## Overview

This project has been successfully upgraded from PyTorch 1.7.0 to PyTorch 2.9.1+ with full compatibility maintained.

## ‚úÖ What Was Changed

### 1. Dependencies Updated (`requirements.txt`)

| Package | Original | Updated |
|---------|----------|---------|
| torch | 1.7.0 | ‚â•2.0.0 |
| transformers | 4.4.2 | ‚â•4.30.0 |
| textworld | 1.4.0 | ‚â•1.5.4 |
| tqdm | 4.60.0 | ‚â•4.65.0 |
| numpy | 1.19.1 | ‚â•1.24.0 |
| python-Levenshtein | 0.12.2 | ‚â•0.21.0 |
| **hf-transfer** | (new) | ‚â•0.1.8 |

### 2. Code Changes

#### AdamW Import Fix
In newer PyTorch/Transformers versions, `AdamW` moved from `transformers` to `torch.optim`.

**Files Updated:**
- `probe_models.py`
- `scripts/train_alchemy.py`
- `scripts/train_textworld.py`
- `scripts/probe_alchemy.py`
- `scripts/probe_textworld.py`

**Change:**
```python
# Old (PyTorch 1.7)
from transformers import AdamW

# New (PyTorch 2.x)
from torch.optim import AdamW
```

#### Regex Escape Sequence Fix
Fixed syntax warning in `data/textworld/tw_dataloader.py`:

```python
# Old
candidates = [f'[ |\n|\'|"][{entity[0].lower()}|{entity[0].upper()}]{entity[1:].lower()}[ |\n|,|\.|!|\?|\'|"]']

# New
candidates = [f'[ |\n|\'|"][{entity[0].lower()}|{entity[0].upper()}]{entity[1:].lower()}[ |\n|,|\\.|!|\\?|\'|"]']
```

## ‚úÖ What Was Tested

All major functionality has been verified:

1. ‚úì Basic imports (torch, transformers, textworld)
2. ‚úì Project-specific imports (probe_models, dataloaders, localizers)
3. ‚úì AdamW optimizer import
4. ‚úì Model/tokenizer initialization (BART, T5)
5. ‚úì Training scripts can run (`train_textworld.py`, `train_alchemy.py`)
6. ‚úì Probe scripts can run (`probe_textworld.py`, `probe_alchemy.py`)

## üì¶ Installation

### Quick Start

```bash
# Create virtual environment (on fast filesystem)
python3 -m venv /tmp/state-probes-venv
source /tmp/state-probes-venv/bin/activate

# Install PyTorch 2.x (CPU version)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# Install dependencies
cd /workspace/state-probes
pip install -r requirements.txt

# Set environment variables
export PYTHONPATH=/workspace/state-probes
export TOKENIZERS_PARALLELISM=true
```

### Or Use the Setup Script

```bash
cd /workspace/state-probes
bash setup_venv.sh
```

## üß™ Verification

Run the test script to verify everything works:

```bash
source /tmp/state-probes-venv/bin/activate
export PYTHONPATH=/workspace/state-probes
export TOKENIZERS_PARALLELISM=true
python test_setup.py
```

Expected output:
```
Testing PyTorch 2.x compatibility...
============================================================
1. Testing basic imports...
   ‚úì PyTorch: 2.9.1+cpu
   ‚úì Transformers: 4.57.1
   ‚úì TextWorld: 1.6.2
   ‚úì NumPy: 2.3.3
...
‚úì All tests passed! Setup is working correctly.
============================================================
```

## üéØ TextWorld Workflow (Your Focus)

### Step 1: Get Data

```bash
wget http://web.mit.edu/bzl/www/tw_data.tar.gz
tar -xzvf tw_data.tar.gz
```

### Step 2: Train Language Model

```bash
python scripts/train_textworld.py \
    --arch bart \
    --data tw_data/simple_traces \
    --gamefile tw_data/simple_games
```

### Step 3: Get Proposition Encodings

```bash
python scripts/get_all_tw_facts.py \
    --data tw_data/simple_traces \
    --gamefile tw_data/simple_games \
    --state_model_arch bart \
    --probe_target belief_facts_pair \
    --state_model_path <path_to_lm_checkpoint> \
    --out_file tw_prop_encodings.pkl
```

### Step 4: Train Probe

```bash
python scripts/probe_textworld.py \
    --arch bart \
    --data tw_data/simple_traces \
    --gamefile tw_data/simple_games \
    --probe_target final.full_belief_facts_pair \
    --encode_tgt_state NL.bart \
    --localizer_type belief_facts_pair_all \
    --probe_type 3linear_classify \
    --probe_agg_method avg \
    --tgt_agg_method avg \
    --lm_save_path <path_to_lm_checkpoint> \
    --ents_to_states_file tw_prop_encodings.pkl \
    --eval_batchsize 256 \
    --batchsize 32
```

### Step 5: Evaluate

```bash
python scripts/probe_textworld.py \
    --eval_only \
    --probe_save_path <path_to_probe_checkpoint> \
    [... same args as training ...]
```

### Step 6: Print Metrics

```bash
python scripts/print_metrics.py \
    --arch bart \
    --domain textworld \
    --pred_files <prediction_file_1>,<prediction_file_2>,...
```

## üîß Key Changes from PyTorch 1.7

### API Compatibility
- **AdamW**: Moved to `torch.optim` (already fixed in code)
- **Transformers API**: Some minor changes in v4.x, but backward compatible
- **Model Loading**: No changes needed; works out of the box

### Performance Notes
- PyTorch 2.x includes many optimizations (torch.compile, etc.)
- For best performance, consider upgrading to CUDA-enabled PyTorch
- CPU version works fine for testing and small experiments

### Breaking Changes (None Affecting This Codebase)
- All changes have been handled in the migration
- No user-facing API changes in this project

## üìù Notes

1. **Virtual Environment Location**: Use fast filesystem (like `/tmp`) for better I/O performance
2. **CUDA Support**: Current setup uses CPU-only PyTorch. For GPU support:
   ```bash
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
   ```
3. **TextWorld Version**: Upgraded to 1.6.2 which has pre-built wheels (no manual Inform7 setup needed)
4. **Python Version**: Tested with Python 3.12, compatible with 3.9+

## üêõ Troubleshooting

### Issue: "No module named 'transformers'"
**Solution**: Make sure virtual environment is activated and dependencies installed

### Issue: "AdamW import error"
**Solution**: Already fixed - code now uses `torch.optim.AdamW`

### Issue: "HF_HUB_ENABLE_HF_TRANSFER error"
**Solution**: Already fixed - `hf-transfer` added to requirements

### Issue: Out of memory
**Solution**: Reduce batch sizes in training/probe scripts or use CPU version

## ‚ú® What's Preserved

All original functionality is preserved:
- ‚úì Alchemy domain support
- ‚úì TextWorld domain support  
- ‚úì All probe types (linear, bilinear, decoder)
- ‚úì All localization methods
- ‚úì Intervention experiments
- ‚úì Metrics and evaluation

## üöÄ Ready to Use

The codebase is now fully functional with PyTorch 2.x. You can start training models and running probes immediately!

